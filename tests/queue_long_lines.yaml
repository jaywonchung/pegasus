- echo "SHORT LINE Testing from {{ hostname }}"
- echo "MEDIUM LINE This is a medium length line from {{ hostname }} that should fit in most terminals without wrapping but is longer than the short one above"
- echo "VERY LONG LINE This is an extremely long line from {{ hostname }} that contains a lot of information and should definitely exceed the terminal width causing wrapping behavior to be tested. It includes details like model_id meta-llama-Llama-3.1-70B-Instruct num_requests 1024 gpu_model B200 num_gpus 2 max_num_seqs 128 max_num_batched_tokens None dataset_path lmarena-ai-arena-human-preference-100k dataset_split train max_concurrency None request_rate inf burstiness 1.0 ignore_eos False max_output_tokens 4096 top_p 0.95 top_k None min_p None temperature 0.8 server_image jaywonchung-vllm-v0.11.1-audio container_runtime docker just_server False percentile_metrics ttft,tpot,itl,e2el metric_percentiles 50,90,95,99 overwrite_results False and many more parameters that go on and on"
- echo "SUPER LONG LINE WITH PATH Spawning vLLM server with command sudo docker run --gpus device 0,1,2,3,4,5,6,7 --ipc host --net host --name benchmark-vllm-01234567 -e HF_TOKEN hf_some_random_token -e HF_HOME /root/.cache/huggingface -e VLLM_LOGGING_LEVEL DEBUG -e VLLM_FLASHINFER_ALLREDUCE_FUSION_THRESHOLDS_MB 2-32-4-32-8-8 -e VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8 1 -e VLLM_LOG_STATS_INTERVAL 1.0 -e VLLM_CACHE_DIR /home/ubuntu/.cache/vllm -v /home/ubuntu/workspace/benchmark/configs/vllm/lm-arena-chat/Qwen/Qwen3-235B-A22B-Instruct-2507-FP8/B200/monolithic.config.yaml /vllm_config/monolithic.config.yaml ro -v /root/.cache/huggingface /data/hfcache -v /home/ubuntu/.cache/vllm /home/ubuntu/.cache/vllm --entrypoint vllm/vllm-openai:v0.11.1-audio vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 --config /vllm_config/monolithic.config.yaml --port 8004 --tensor-parallel-size 2 --max-num-seqs 128 from {{ hostname }}"
- |
  echo "=== Testing line wrapping behavior ==="
  echo "1. Short"
  echo "2. A bit longer line from {{ hostname }}"
  echo "3. EXTREMELY LONG LINE workload LMArenaChat base_dir PosixPath run/llm seed 48105 model_id meta-llama/Llama-3.1-70B-Instruct num_requests 1024 gpu_model B200 num_gpus 2 max_num_seqs 128 max_num_batched_tokens None dataset_path lmarena-ai/arena-human-preference-100k dataset_split train max_concurrency None request_rate inf burstiness 1.0 ignore_eos False max_output_tokens 4096 top_p 0.95 top_k None min_p None temperature 0.8 server_image jaywonchung/vllm v0.11.1-audio container_runtime docker just_server False percentile_metrics ttft,tpot,itl,e2el metric_percentiles 50,90,95,99 overwrite_results False"
  echo "4. Back to short"
  echo "5. Another very long line that simulates real output INFO mlenergy.llm.benchmark 1178 Spawning vLLM server with command sudo docker run --gpus device 6,7 --ipc host --net host --name benchmark-vllm-67 -e HF_TOKEN hf_some_random_token -e HF_HOME /root/.cache/huggingface -e VLLM_LOGGING_LEVEL DEBUG -e VLLM_LOG_STATS_INTERVAL 1.0 -e VLLM_CACHE_DIR /home/ubuntu/.cache/vllm -v /home/ubuntu/workspace/benchmark/configs/vllm/lm-arena-chat/meta-llama/Llama-3.1-70B-Instruct/B200/monolithic.config.yaml /vllm_config/monolithic.config.yaml ro -v /root/.cache/huggingface:/root/.cache/huggingface -v /home/ubuntu/.cache/vllm /home/ubuntu/.cache/vllm --entrypoint vllm/vllm-openai:v0.11.1-audio vllm serve meta-llama/Llama-3.1-70B-Instruct --config /vllm_config/monolithic.config.yaml --port 8006 --tensor-parallel-size 2 --max-num-seqs 128"
  echo "6. Short again"
  echo "=== Test complete ==="
